"""
MedRAG + LangGraph é›†æˆæ–¹æ¡ˆ
æ•´åˆ MedRAG å·¥å…·åŒ…åˆ° MediAgents-LangGraph æ¶æ„ä¸­
"""

from typing import TypedDict, Annotated, List, Dict, Any, Optional, Tuple
from typing_extensions import Literal
from langgraph.graph import StateGraph, END
import operator
import asyncio
from dataclasses import dataclass
from enum import Enum
import json
import time
from pydantic import BaseModel, Field

# å¯¼å…¥ç°æœ‰ç»„ä»¶
from langgraph_implementation import MedicalWorkflowState, MedicalQuery, QueryType, DifficultyLevel, AgentResponse
from utils import Agent, setup_model

# MedRAG ç›¸å…³å¯¼å…¥ (éœ€è¦å…ˆå®‰è£… MedRAG)
# pip install git+https://github.com/Teddy-XiongGZ/MedRAG.git
try:
    # å‡è®¾ MedRAG çš„ä¸»è¦ç»„ä»¶
    from medrag import MedRAG
    from medrag.retrieval import BM25Retriever, ContrieverRetriever, MedCPTRetriever
    from medrag.corpora import PubMedCorpus, StatPearlsCorpus, WikipediaCorpus
    MEDRAG_AVAILABLE = True
except ImportError:
    print("âš ï¸ MedRAG æœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå®ç°")
    MEDRAG_AVAILABLE = False

@dataclass
class RAGResult:
    """RAGæ£€ç´¢ç»“æœ"""
    query: str
    retrieved_passages: List[Dict[str, Any]]
    relevance_scores: List[float]
    sources: List[Dict[str, str]]
    retrieval_time: float
    total_passages: int

class MedRAGConfig(BaseModel):
    """MedRAG é…ç½®"""
    corpus_name: str = "pubmed"  # pubmed, statpearls, textbooks, wikipedia
    retriever_name: str = "medcpt"  # bm25, contriever, specter, medcpt
    retrieve_k: int = 5
    enable_rerank: bool = True
    max_passage_length: int = 500
    enable_followup_query: bool = True  # i-MedRAG feature

class EnhancedMedicalWorkflowState(MedicalWorkflowState):
    """å¢å¼ºçš„åŒ»å­¦å·¥ä½œæµçŠ¶æ€ - é›†æˆ RAG åŠŸèƒ½"""
    # RAG ç›¸å…³çŠ¶æ€
    rag_results: Annotated[List[RAGResult], operator.add]
    knowledge_context: Optional[str]
    followup_queries: Annotated[List[str], operator.add]
    evidence_quality_score: Optional[float]

class MedRAGIntegratedSystem:
    """é›†æˆ MedRAG çš„åŒ»å­¦æ™ºèƒ½ä½“ç³»ç»Ÿ"""
    
    def __init__(self, 
                 model_name: str = "gemini-2.5-flash-lite-preview-06-17",
                 medrag_config: Optional[MedRAGConfig] = None):
        self.model_name = model_name
        self.medrag_config = medrag_config or MedRAGConfig()
        
        # è®¾ç½®æ¨¡å‹
        self.setup_successful = setup_model(model_name)
        if not self.setup_successful:
            raise ValueError(f"Failed to setup model: {model_name}")
        
        # åˆå§‹åŒ– MedRAG ç»„ä»¶
        self.medrag_system = self._initialize_medrag()
        
        # åˆ›å»ºå¢å¼ºçš„å·¥ä½œæµ
        self.workflow = self._create_enhanced_workflow()
    
    def _initialize_medrag(self) -> Optional[Any]:
        """åˆå§‹åŒ– MedRAG ç³»ç»Ÿ"""
        if not MEDRAG_AVAILABLE:
            print("ğŸ”„ ä½¿ç”¨æ¨¡æ‹Ÿ MedRAG å®ç°")
            return self._create_mock_medrag()
        
        try:
            print(f"ğŸ”„ åˆå§‹åŒ– MedRAG - Corpus: {self.medrag_config.corpus_name}, Retriever: {self.medrag_config.retriever_name}")
            
            # é€‰æ‹©è¯­æ–™åº“
            corpus_map = {
                "pubmed": PubMedCorpus,
                "statpearls": StatPearlsCorpus, 
                "wikipedia": WikipediaCorpus
            }
            
            # é€‰æ‹©æ£€ç´¢å™¨
            retriever_map = {
                "bm25": BM25Retriever,
                "contriever": ContrieverRetriever,
                "medcpt": MedCPTRetriever
            }
            
            corpus = corpus_map.get(self.medrag_config.corpus_name, PubMedCorpus)()
            retriever = retriever_map.get(self.medrag_config.retriever_name, MedCPTRetriever)()
            
            medrag_system = MedRAG(
                corpus=corpus,
                retriever=retriever,
                llm_name=self.model_name
            )
            
            print("âœ… MedRAG åˆå§‹åŒ–æˆåŠŸ")
            return medrag_system
            
        except Exception as e:
            print(f"âš ï¸ MedRAG åˆå§‹åŒ–å¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°æ¨¡æ‹Ÿå®ç°")
            return self._create_mock_medrag()
    
    def _create_mock_medrag(self):
        """åˆ›å»º MedRAG çš„æ¨¡æ‹Ÿå®ç°"""
        class MockMedRAG:
            def retrieve(self, query: str, k: int = 5) -> Tuple[List[str], List[float]]:
                # æ¨¡æ‹Ÿæ£€ç´¢ç»“æœ
                mock_passages = [
                    f"Medical literature passage 1 relevant to: {query[:50]}...",
                    f"Clinical guideline excerpt 2 for: {query[:50]}...",
                    f"Research finding 3 related to: {query[:50]}...",
                    f"Treatment recommendation 4 about: {query[:50]}...",
                    f"Diagnostic criteria 5 for: {query[:50]}..."
                ]
                
                mock_scores = [0.95, 0.87, 0.82, 0.76, 0.71]
                return mock_passages[:k], mock_scores[:k]
            
            def generate_followup_queries(self, original_query: str) -> List[str]:
                # æ¨¡æ‹Ÿåç»­æŸ¥è¯¢ç”Ÿæˆ
                return [
                    f"What are the differential diagnoses for {original_query[:30]}?",
                    f"What are the latest treatment guidelines for {original_query[:30]}?",
                    f"What are the risk factors associated with {original_query[:30]}?"
                ]
        
        return MockMedRAG()
    
    def _create_enhanced_workflow(self) -> StateGraph:
        """åˆ›å»ºå¢å¼ºçš„å·¥ä½œæµï¼Œé›†æˆ RAG åŠŸèƒ½"""
        workflow = StateGraph(EnhancedMedicalWorkflowState)
        
        # æ·»åŠ èŠ‚ç‚¹ - åŒ…å« RAG å¢å¼ºèŠ‚ç‚¹
        workflow.add_node("difficulty_assessor", self._assess_difficulty)
        workflow.add_node("rag_retriever", self._retrieve_medical_knowledge)
        workflow.add_node("knowledge_synthesizer", self._synthesize_knowledge)
        workflow.add_node("expert_recruiter", self._recruit_rag_informed_experts)
        workflow.add_node("rag_enhanced_basic", self._rag_enhanced_basic_processing)
        workflow.add_node("rag_enhanced_intermediate", self._rag_enhanced_intermediate_processing)
        workflow.add_node("rag_enhanced_advanced", self._rag_enhanced_advanced_processing)
        workflow.add_node("evidence_evaluator", self._evaluate_evidence_quality)
        workflow.add_node("final_synthesizer", self._synthesize_final_response)
        workflow.add_node("quality_validator", self._validate_response_quality)
        
        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("difficulty_assessor")
        
        # å·¥ä½œæµè·¯å¾„ï¼šè¯„ä¼° -> RAGæ£€ç´¢ -> çŸ¥è¯†ç»¼åˆ -> ä¸“å®¶æ‹›å‹Ÿ -> å¤„ç† -> è¯„ä¼° -> ç»¼åˆ -> éªŒè¯
        workflow.add_edge("difficulty_assessor", "rag_retriever")
        workflow.add_edge("rag_retriever", "knowledge_synthesizer") 
        workflow.add_edge("knowledge_synthesizer", "expert_recruiter")
        
        # æ¡ä»¶è·¯ç”±åˆ°ä¸åŒçš„å¤„ç†å™¨
        workflow.add_conditional_edges(
            "expert_recruiter",
            self._route_to_rag_processor,
            {
                "basic": "rag_enhanced_basic",
                "intermediate": "rag_enhanced_intermediate",
                "advanced": "rag_enhanced_advanced"
            }
        )
        
        # æ‰€æœ‰å¤„ç†å™¨éƒ½è¿æ¥åˆ°è¯æ®è¯„ä¼°
        workflow.add_edge("rag_enhanced_basic", "evidence_evaluator")
        workflow.add_edge("rag_enhanced_intermediate", "evidence_evaluator")
        workflow.add_edge("rag_enhanced_advanced", "evidence_evaluator")
        
        # æœ€ç»ˆæµç¨‹
        workflow.add_edge("evidence_evaluator", "final_synthesizer")
        workflow.add_edge("final_synthesizer", "quality_validator")
        workflow.add_edge("quality_validator", END)
        
        return workflow.compile()
    
    def _assess_difficulty(self, state: EnhancedMedicalWorkflowState) -> EnhancedMedicalWorkflowState:
        """è¯„ä¼°æŸ¥è¯¢éš¾åº¦ - å¤ç”¨åŸæœ‰é€»è¾‘"""
        print("[MedRAG-LangGraph] ğŸ” è¯„ä¼°æŸ¥è¯¢éš¾åº¦...")
        
        query = state["query"]
        start_time = time.time()
        
        from utils import determine_difficulty
        difficulty_level, input_tokens, output_tokens = determine_difficulty(
            query.query_text, 
            "adaptive", 
            self.model_name
        )
        
        processing_time = time.time() - start_time
        
        print(f"[MedRAG-LangGraph] âœ… éš¾åº¦è¯„ä¼°: {difficulty_level} (è€—æ—¶: {processing_time:.2f}s)")
        
        return {
            **state,
            "difficulty_level": DifficultyLevel(difficulty_level),
            "total_input_tokens": input_tokens,
            "total_output_tokens": output_tokens,
            "rag_results": [],
            "followup_queries": [],
            "processing_metadata": {
                "difficulty_assessment_time": processing_time
            }
        }
    
    def _retrieve_medical_knowledge(self, state: EnhancedMedicalWorkflowState) -> EnhancedMedicalWorkflowState:
        """ä½¿ç”¨ MedRAG æ£€ç´¢åŒ»å­¦çŸ¥è¯†"""
        print("[MedRAG-LangGraph] ğŸ” æ£€ç´¢åŒ»å­¦çŸ¥è¯†...")
        
        query = state["query"]
        start_time = time.time()
        
        # ä¸»æŸ¥è¯¢æ£€ç´¢
        passages, scores = self.medrag_system.retrieve(
            query.query_text, 
            k=self.medrag_config.retrieve_k
        )
        
        # æ„å»ºæ£€ç´¢ç»“æœ
        retrieved_passages = []
        sources = []
        
        for i, (passage, score) in enumerate(zip(passages, scores)):
            retrieved_passages.append({
                "content": passage,
                "score": score,
                "rank": i + 1,
                "source_type": self.medrag_config.corpus_name
            })
            
            sources.append({
                "title": f"{self.medrag_config.corpus_name.title()} Source {i+1}",
                "content": passage[:100] + "...",
                "score": score,
                "url": f"#{self.medrag_config.corpus_name}_source_{i+1}"
            })
        
        # ç”Ÿæˆåç»­æŸ¥è¯¢ (i-MedRAG åŠŸèƒ½)
        followup_queries = []
        if self.medrag_config.enable_followup_query:
            followup_queries = self.medrag_system.generate_followup_queries(query.query_text)
        
        retrieval_time = time.time() - start_time
        
        rag_result = RAGResult(
            query=query.query_text,
            retrieved_passages=retrieved_passages,
            relevance_scores=scores,
            sources=sources,
            retrieval_time=retrieval_time,
            total_passages=len(passages)
        )
        
        print(f"[MedRAG-LangGraph] âœ… çŸ¥è¯†æ£€ç´¢å®Œæˆ: {len(passages)} ä¸ªç›¸å…³ç‰‡æ®µ (è€—æ—¶: {retrieval_time:.2f}s)")
        if followup_queries:
            print(f"[MedRAG-LangGraph] ğŸ’¡ ç”Ÿæˆ {len(followup_queries)} ä¸ªåç»­æŸ¥è¯¢")
        
        return {
            **state,
            "rag_results": [rag_result],
            "followup_queries": followup_queries
        }
    
    def _synthesize_knowledge(self, state: EnhancedMedicalWorkflowState) -> EnhancedMedicalWorkflowState:
        """ç»¼åˆæ£€ç´¢åˆ°çš„çŸ¥è¯†"""
        print("[MedRAG-LangGraph] ğŸ§  ç»¼åˆåŒ»å­¦çŸ¥è¯†...")
        
        rag_results = state["rag_results"]
        if not rag_results:
            return {**state, "knowledge_context": ""}
        
        # æå–æœ€ç›¸å…³çš„çŸ¥è¯†
        all_passages = []
        for result in rag_results:
            # é€‰æ‹©è¯„åˆ†æœ€é«˜çš„ç‰‡æ®µ
            top_passages = sorted(
                result.retrieved_passages, 
                key=lambda x: x["score"], 
                reverse=True
            )[:3]  # å–å‰3ä¸ªæœ€ç›¸å…³çš„
            
            all_passages.extend([p["content"] for p in top_passages])
        
        # æ„å»ºçŸ¥è¯†ä¸Šä¸‹æ–‡
        knowledge_context = "\n\n".join([
            f"**Medical Knowledge {i+1}:**\n{passage}"
            for i, passage in enumerate(all_passages)
        ])
        
        print(f"[MedRAG-LangGraph] âœ… çŸ¥è¯†ç»¼åˆå®Œæˆ: {len(all_passages)} ä¸ªå…³é”®çŸ¥è¯†ç‰‡æ®µ")
        
        return {
            **state,
            "knowledge_context": knowledge_context
        }
    
    def _recruit_rag_informed_experts(self, state: EnhancedMedicalWorkflowState) -> EnhancedMedicalWorkflowState:
        """åŸºäº RAG çŸ¥è¯†æ‹›å‹Ÿä¸“å®¶"""
        print(f"[MedRAG-LangGraph] ğŸ‘¥ åŸºäºæ£€ç´¢çŸ¥è¯†æ‹›å‹Ÿ {state['difficulty_level']} çº§åˆ«ä¸“å®¶...")
        
        difficulty = state["difficulty_level"]
        query = state["query"]
        knowledge_context = state["knowledge_context"]
        
        # åŸºäºæ£€ç´¢åˆ°çš„çŸ¥è¯†ä¼˜åŒ–ä¸“å®¶é…ç½®
        expert_configs = self._get_rag_informed_expert_configs(difficulty, query, knowledge_context)
        
        recruited_experts = []
        for config in expert_configs:
            recruited_experts.append({
                "role": config["role"],
                "expertise": config["expertise"],
                "specialization": config.get("specialization", ""),
                "weight": config.get("weight", 1.0),
                "agent_id": f"{config['role']}_{len(recruited_experts)}",
                "rag_informed": True
            })
        
        print(f"[MedRAG-LangGraph] âœ… æ‹›å‹Ÿå®Œæˆ: {len(recruited_experts)} ä½çŸ¥è¯†å¢å¼ºä¸“å®¶")
        
        return {
            **state,
            "recruited_experts": recruited_experts
        }
    
    def _get_rag_informed_expert_configs(self, difficulty: DifficultyLevel, query: MedicalQuery, knowledge_context: str) -> List[Dict]:
        """åŸºäºRAGçŸ¥è¯†ç¡®å®šä¸“å®¶é…ç½®"""
        # åˆ†æçŸ¥è¯†å†…å®¹ç¡®å®šéœ€è¦çš„ä¸“ä¸šé¢†åŸŸ
        knowledge_keywords = knowledge_context.lower()
        
        base_experts = [
            {"role": "RAG-Enhanced Clinician", "expertise": "evidence-based clinical decision making with access to latest medical literature"},
            {"role": "Knowledge Synthesizer", "expertise": "integrating multiple sources of medical evidence"},
            {"role": "Guidelines Specialist", "expertise": "interpreting and applying clinical practice guidelines"}
        ]
        
        # æ ¹æ®çŸ¥è¯†å†…å®¹æ·»åŠ ä¸“ä¸šåŒ–ä¸“å®¶
        if any(term in knowledge_keywords for term in ["cardiac", "heart", "coronary"]):
            base_experts.append({"role": "Cardiologist", "expertise": "cardiovascular medicine and cardiac care"})
        
        if any(term in knowledge_keywords for term in ["neuro", "brain", "cognitive"]):
            base_experts.append({"role": "Neurologist", "expertise": "neurological disorders and brain health"})
        
        if any(term in knowledge_keywords for term in ["infection", "antibiotic", "pathogen"]):
            base_experts.append({"role": "Infectious Disease Specialist", "expertise": "infectious diseases and antimicrobial therapy"})
        
        # æ ¹æ®éš¾åº¦è°ƒæ•´ä¸“å®¶æ•°é‡
        if difficulty == DifficultyLevel.ADVANCED:
            base_experts.append({"role": "Research Physician", "expertise": "translating cutting-edge research into clinical practice"})
        
        return base_experts[:6]  # é™åˆ¶ä¸“å®¶æ•°é‡
    
    def _rag_enhanced_basic_processing(self, state: EnhancedMedicalWorkflowState) -> EnhancedMedicalWorkflowState:
        """RAG å¢å¼ºçš„åŸºç¡€å¤„ç†"""
        print("[MedRAG-LangGraph] ğŸ”„ æ‰§è¡Œ RAG å¢å¼ºåŸºç¡€å¤„ç†...")
        
        query = state["query"]
        experts = state["recruited_experts"]
        knowledge_context = state["knowledge_context"]
        
        agent_responses = []
        total_input = total_output = 0
        
        for expert in experts:
            # åˆ›å»ºå…·æœ‰RAGçŸ¥è¯†çš„ä¸“å®¶
            rag_enhanced_instruction = f"""
            You are a {expert['role']} with expertise in {expert['expertise']}.
            
            You have access to the following relevant medical knowledge:
            {knowledge_context}
            
            Use this evidence-based information to provide your professional medical opinion.
            """
            
            agent = Agent(
                instruction=rag_enhanced_instruction,
                role=expert['role'],
                model_info=self.model_name
            )
            
            start_time = time.time()
            response_text = agent.chat(
                f"Medical Query: {query.query_text}\n\nBased on the provided medical evidence, give your professional analysis and recommendation (limit to 200 words):"
            )
            processing_time = time.time() - start_time
            
            agent_response = AgentResponse(
                agent_id=expert['agent_id'],
                role=expert['role'],
                response=response_text,
                confidence=0.85,  # RAGå¢å¼ºåç½®ä¿¡åº¦æé«˜
                tokens_used={
                    "input": agent.total_input_tokens,
                    "output": agent.total_output_tokens
                },
                processing_time=processing_time
            )
            
            agent_responses.append(agent_response)
            total_input += agent.total_input_tokens
            total_output += agent.total_output_tokens
        
        print(f"[MedRAG-LangGraph] âœ… RAG å¢å¼ºåŸºç¡€å¤„ç†å®Œæˆ: {len(agent_responses)} ä¸ªä¸“å®¶æ„è§")
        
        return {
            **state,
            "agent_responses": agent_responses,
            "total_input_tokens": total_input,
            "total_output_tokens": total_output
        }
    
    def _rag_enhanced_intermediate_processing(self, state: EnhancedMedicalWorkflowState) -> EnhancedMedicalWorkflowState:
        """RAG å¢å¼ºçš„ä¸­çº§å¤„ç†"""
        print("[MedRAG-LangGraph] ğŸ”„ æ‰§è¡Œ RAG å¢å¼ºä¸­çº§å¤„ç†...")
        
        # ç»“åˆ RAG çŸ¥è¯†çš„ä¸“å®¶åä½œè®¨è®º
        query = state["query"]
        experts = state["recruited_experts"] 
        knowledge_context = state["knowledge_context"]
        
        # ç¬¬ä¸€è½®ï¼šåŸºäºRAGçŸ¥è¯†çš„åˆå§‹æ„è§
        initial_responses = self._rag_informed_initial_opinions(query, experts, knowledge_context)
        
        # ç¬¬äºŒè½®ï¼šçŸ¥è¯†å¢å¼ºçš„ä¸“å®¶è®¨è®º
        discussion_responses = self._rag_enhanced_expert_discussion(query, experts, initial_responses, knowledge_context)
        
        all_responses = initial_responses + discussion_responses
        
        total_input = sum(r.tokens_used["input"] for r in all_responses)
        total_output = sum(r.tokens_used["output"] for r in all_responses)
        
        print(f"[MedRAG-LangGraph] âœ… RAG å¢å¼ºä¸­çº§å¤„ç†å®Œæˆ: {len(all_responses)} è½®ä¸“å®¶äº¤äº’")
        
        return {
            **state,
            "agent_responses": all_responses,
            "total_input_tokens": total_input,
            "total_output_tokens": total_output,
            "interaction_history": [
                {"round": 1, "type": "rag_informed_opinions", "count": len(initial_responses)},
                {"round": 2, "type": "rag_enhanced_discussion", "count": len(discussion_responses)}
            ]
        }
    
    def _rag_enhanced_advanced_processing(self, state: EnhancedMedicalWorkflowState) -> EnhancedMedicalWorkflowState:
        """RAG å¢å¼ºçš„é«˜çº§å¤„ç†"""
        print("[MedRAG-LangGraph] ğŸ”„ æ‰§è¡Œ RAG å¢å¼ºé«˜çº§å¤„ç†...")
        
        query = state["query"]
        experts = state["recruited_experts"]
        knowledge_context = state["knowledge_context"]
        
        # å¤šé˜¶æ®µå¤„ç†ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½ä½¿ç”¨ RAG çŸ¥è¯†
        responses = []
        
        # é˜¶æ®µ1ï¼šä¸“ä¸šåˆ†æ (åŸºäºRAG)
        specialist_responses = self._rag_specialist_analysis(query, experts, knowledge_context)
        responses.extend(specialist_responses)
        
        # é˜¶æ®µ2ï¼šè·¨å­¦ç§‘è®¨è®º (RAGå¢å¼º)
        interdisciplinary_responses = self._rag_interdisciplinary_discussion(
            query, experts, specialist_responses, knowledge_context
        )
        responses.extend(interdisciplinary_responses)
        
        # é˜¶æ®µ3ï¼šåè°ƒå‘˜ç»¼åˆ (æ•´åˆæ‰€æœ‰RAGçŸ¥è¯†)
        coordinator_response = self._rag_coordinator_synthesis(
            query, experts, responses, knowledge_context
        )
        responses.append(coordinator_response)
        
        total_input = sum(r.tokens_used["input"] for r in responses)
        total_output = sum(r.tokens_used["output"] for r in responses)
        
        print(f"[MedRAG-LangGraph] âœ… RAG å¢å¼ºé«˜çº§å¤„ç†å®Œæˆ: 3 é˜¶æ®µå…± {len(responses)} æ¬¡äº¤äº’")
        
        return {
            **state,
            "agent_responses": responses,
            "total_input_tokens": total_input,
            "total_output_tokens": total_output,
            "interaction_history": [
                {"phase": 1, "type": "rag_specialist_analysis", "count": len(specialist_responses)},
                {"phase": 2, "type": "rag_interdisciplinary_discussion", "count": len(interdisciplinary_responses)},
                {"phase": 3, "type": "rag_coordinator_synthesis", "count": 1}
            ]
        }
    
    def _evaluate_evidence_quality(self, state: EnhancedMedicalWorkflowState) -> EnhancedMedicalWorkflowState:
        """è¯„ä¼°è¯æ®è´¨é‡"""
        print("[MedRAG-LangGraph] ğŸ“Š è¯„ä¼°è¯æ®è´¨é‡...")
        
        rag_results = state["rag_results"]
        agent_responses = state["agent_responses"]
        
        # è®¡ç®—è¯æ®è´¨é‡è¯„åˆ†
        evidence_scores = []
        
        if rag_results:
            for result in rag_results:
                # åŸºäºæ£€ç´¢è¯„åˆ†å’Œæ•°é‡è®¡ç®—è´¨é‡
                avg_relevance = sum(result.relevance_scores) / len(result.relevance_scores)
                passage_count_factor = min(result.total_passages / 5, 1.0)  # å½’ä¸€åŒ–åˆ°[0,1]
                
                quality_score = (avg_relevance * 0.7) + (passage_count_factor * 0.3)
                evidence_scores.append(quality_score)
        
        # ç»¼åˆè¯æ®è´¨é‡è¯„åˆ†
        overall_evidence_quality = sum(evidence_scores) / len(evidence_scores) if evidence_scores else 0.5
        
        print(f"[MedRAG-LangGraph] ğŸ“Š è¯æ®è´¨é‡è¯„ä¼°å®Œæˆ: {overall_evidence_quality:.2f}")
        
        return {
            **state,
            "evidence_quality_score": overall_evidence_quality
        }
    
    def _synthesize_final_response(self, state: EnhancedMedicalWorkflowState) -> EnhancedMedicalWorkflowState:
        """ç»¼åˆæœ€ç»ˆå“åº” - RAGå¢å¼ºç‰ˆ"""
        print("[MedRAG-LangGraph] ğŸ¯ ç»¼åˆ RAG å¢å¼ºçš„æœ€ç»ˆåŒ»å­¦å»ºè®®...")
        
        query = state["query"]
        agent_responses = state["agent_responses"] 
        knowledge_context = state["knowledge_context"]
        evidence_quality = state["evidence_quality_score"]
        rag_results = state["rag_results"]
        
        # åˆ›å»ºRAGå¢å¼ºçš„ç»¼åˆæ™ºèƒ½ä½“
        rag_enhanced_synthesizer = Agent(
            instruction=f"""
            You are a senior medical consultant with access to comprehensive medical literature and evidence.
            
            Available Medical Evidence:
            {knowledge_context}
            
            Your role is to synthesize multiple expert opinions with evidence-based medical knowledge to provide the most accurate and comprehensive medical recommendation.
            """,
            role="RAG-Enhanced Medical Synthesizer",
            model_info=self.model_name
        )
        
        # æ„å»ºç»¼åˆæç¤º
        expert_opinions = "\n\n".join([
            f"**{resp.role} Opinion (Confidence: {resp.confidence:.2f}):**\n{resp.response}"
            for resp in agent_responses
        ])
        
        sources_summary = ""
        if rag_results and rag_results[0].sources:
            sources_summary = "\n\n**Evidence Sources:**\n" + "\n".join([
                f"- {source['title']}: {source['content']}"
                for source in rag_results[0].sources[:3]  # æ˜¾ç¤ºå‰3ä¸ªæ¥æº
            ])
        
        synthesis_prompt = f"""
        Medical Query: {query.query_text}
        
        Expert Opinions:
        {expert_opinions}
        
        {sources_summary}
        
        Evidence Quality Score: {evidence_quality:.2f}/1.0
        
        Based on the expert opinions and the provided medical evidence, provide a comprehensive, evidence-based medical recommendation that:
        
        1. **Clinical Assessment**: Synthesize key diagnostic and clinical insights
        2. **Evidence-Based Recommendations**: Provide recommendations backed by the available evidence
        3. **Risk Considerations**: Address any potential risks or contraindications
        4. **Follow-up Actions**: Suggest appropriate follow-up measures
        5. **Limitations**: Acknowledge any limitations in the current evidence
        
        Structure your response clearly and limit to 500 words. Include confidence indicators where appropriate.
        """
        
        start_time = time.time()
        final_response = rag_enhanced_synthesizer.chat(synthesis_prompt)
        synthesis_time = time.time() - start_time
        
        # RAGå¢å¼ºçš„ç½®ä¿¡åº¦è®¡ç®—
        base_confidence = sum(r.confidence for r in agent_responses) / len(agent_responses)
        evidence_boost = evidence_quality * 0.1  # è¯æ®è´¨é‡å¸¦æ¥çš„ç½®ä¿¡åº¦æå‡
        final_confidence = min(base_confidence + evidence_boost, 0.95)
        
        print(f"[MedRAG-LangGraph] âœ… RAG å¢å¼ºæœ€ç»ˆå»ºè®®å®Œæˆ (ç½®ä¿¡åº¦: {final_confidence:.2f})")
        
        return {
            **state,
            "final_decision": final_response,
            "confidence_score": final_confidence,
            "total_input_tokens": rag_enhanced_synthesizer.total_input_tokens,
            "total_output_tokens": rag_enhanced_synthesizer.total_output_tokens,
            "processing_metadata": {
                **state.get("processing_metadata", {}),
                "synthesis_time": synthesis_time,
                "evidence_enhanced": True,
                "evidence_quality_score": evidence_quality,
                "total_sources": len(rag_results[0].sources) if rag_results else 0
            }
        }
    
    def _validate_response_quality(self, state: EnhancedMedicalWorkflowState) -> EnhancedMedicalWorkflowState:
        """RAGå¢å¼ºçš„è´¨é‡éªŒè¯"""
        print("[MedRAG-LangGraph] âœ… æ‰§è¡Œ RAG å¢å¼ºè´¨é‡éªŒè¯...")
        
        final_decision = state["final_decision"]
        confidence = state["confidence_score"] 
        evidence_quality = state["evidence_quality_score"]
        
        # å¢å¼ºçš„è´¨é‡æŒ‡æ ‡
        quality_metrics = {
            "response_length": len(final_decision.split()),
            "confidence_score": confidence,
            "evidence_quality_score": evidence_quality,
            "expert_count": len(state["agent_responses"]),
            "sources_count": len(state["rag_results"][0].sources) if state["rag_results"] else 0,
            "processing_complete": final_decision is not None,
            "rag_enhanced": True
        }
        
        # RAGå¢å¼ºçš„è´¨é‡æ£€æŸ¥æ ‡å‡†
        quality_passed = (
            quality_metrics["response_length"] > 100 and
            quality_metrics["confidence_score"] > 0.6 and
            quality_metrics["evidence_quality_score"] > 0.3 and
            quality_metrics["sources_count"] > 0 and
            quality_metrics["processing_complete"]
        )
        
        print(f"[MedRAG-LangGraph] ğŸ“Š RAG å¢å¼ºè´¨é‡éªŒè¯ - {'é€šè¿‡' if quality_passed else 'éœ€è¦æ”¹è¿›'}")
        
        return {
            **state,
            "processing_metadata": {
                **state.get("processing_metadata", {}),
                "quality_metrics": quality_metrics,
                "quality_passed": quality_passed,
                "rag_enhanced_validation": True
            }
        }
    
    # è·¯ç”±å‡½æ•°
    def _route_to_rag_processor(self, state: EnhancedMedicalWorkflowState) -> Literal["basic", "intermediate", "advanced"]:
        """è·¯ç”±åˆ°RAGå¢å¼ºçš„å¤„ç†å™¨"""
        difficulty = state["difficulty_level"]
        print(f"[MedRAG-LangGraph] ğŸ”€ è·¯ç”±åˆ° RAG å¢å¼º {difficulty.value} å¤„ç†å™¨")
        return difficulty.value
    
    # RAGå¢å¼ºçš„è¾…åŠ©æ–¹æ³•
    def _rag_informed_initial_opinions(self, query: MedicalQuery, experts: List[Dict], knowledge_context: str) -> List[AgentResponse]:
        """åŸºäºRAGçŸ¥è¯†çš„åˆå§‹æ„è§æ”¶é›†"""
        responses = []
        for expert in experts:
            agent = Agent(
                instruction=f"""
                You are a {expert['role']} with expertise in {expert['expertise']}.
                
                Relevant Medical Evidence:
                {knowledge_context}
                
                Base your analysis on the provided evidence and your professional expertise.
                """,
                role=expert['role'],
                model_info=self.model_name
            )
            
            start_time = time.time()
            response_text = agent.chat(
                f"Medical Query: {query.query_text}\n\nProvide your evidence-based analysis (limit to 150 words):"
            )
            processing_time = time.time() - start_time
            
            responses.append(AgentResponse(
                agent_id=expert['agent_id'],
                role=expert['role'],
                response=response_text,
                confidence=0.8,  # RAGå¢å¼ºæé«˜ç½®ä¿¡åº¦
                tokens_used={"input": agent.total_input_tokens, "output": agent.total_output_tokens},
                processing_time=processing_time
            ))
        
        return responses
    
    def _rag_enhanced_expert_discussion(self, query: MedicalQuery, experts: List[Dict], 
                                       initial_responses: List[AgentResponse], 
                                       knowledge_context: str) -> List[AgentResponse]:
        """RAGå¢å¼ºçš„ä¸“å®¶è®¨è®º"""
        discussion_responses = []
        
        other_opinions = "\n".join([
            f"{resp.role}: {resp.response[:100]}..."
            for resp in initial_responses
        ])
        
        for i, expert in enumerate(experts):
            agent = Agent(
                instruction=f"""
                You are a {expert['role']} in a medical consultation with access to comprehensive medical literature.
                
                Available Evidence:
                {knowledge_context}
                """,
                role=expert['role'],
                model_info=self.model_name
            )
            
            discussion_prompt = f"""
            Medical Query: {query.query_text}
            
            Colleague Opinions:
            {other_opinions}
            
            Based on the evidence and your colleagues' insights, provide your refined analysis (limit to 100 words):
            """
            
            start_time = time.time()
            response_text = agent.chat(discussion_prompt)
            processing_time = time.time() - start_time
            
            discussion_responses.append(AgentResponse(
                agent_id=f"{expert['agent_id']}_discussion",
                role=f"{expert['role']} (Evidence-Enhanced Discussion)",
                response=response_text,
                confidence=0.85,
                tokens_used={"input": agent.total_input_tokens, "output": agent.total_output_tokens},
                processing_time=processing_time
            ))
        
        return discussion_responses
    
    # å…¶ä»– RAG å¢å¼ºæ–¹æ³•çš„å®ç°...
    def _rag_specialist_analysis(self, query, experts, knowledge_context):
        return self._rag_informed_initial_opinions(query, experts, knowledge_context)
    
    def _rag_interdisciplinary_discussion(self, query, experts, previous_responses, knowledge_context):
        return self._rag_enhanced_expert_discussion(query, experts, previous_responses, knowledge_context)
    
    def _rag_coordinator_synthesis(self, query, experts, all_responses, knowledge_context):
        coordinator = Agent(
            instruction=f"""
            You are a senior medical coordinator with access to comprehensive medical evidence.
            
            Available Evidence:
            {knowledge_context}
            """,
            role="RAG-Enhanced MDT Coordinator",
            model_info=self.model_name
        )
        
        all_opinions = "\n\n".join([
            f"**{resp.role}:**\n{resp.response}"
            for resp in all_responses
        ])
        
        coordinator_prompt = f"""
        Medical Query: {query.query_text}
        
        Team Analysis:
        {all_opinions}
        
        As coordinator with access to medical evidence, provide final evidence-based recommendation (limit to 200 words):
        """
        
        start_time = time.time()
        response_text = coordinator.chat(coordinator_prompt)
        processing_time = time.time() - start_time
        
        return AgentResponse(
            agent_id="rag_enhanced_coordinator",
            role="RAG-Enhanced MDT Coordinator",
            response=response_text,
            confidence=0.9,
            tokens_used={"input": coordinator.total_input_tokens, "output": coordinator.total_output_tokens},
            processing_time=processing_time
        )
    
    async def process_medical_query(self, query: MedicalQuery) -> Dict[str, Any]:
        """å¤„ç†åŒ»å­¦æŸ¥è¯¢çš„ä¸»å…¥å£ - RAGå¢å¼ºç‰ˆ"""
        print(f"\n[MedRAG-LangGraph] ğŸš€ å¼€å§‹ RAG å¢å¼ºåŒ»å­¦æŸ¥è¯¢å¤„ç†: {query.query_text[:50]}...")
        
        # åˆå§‹åŒ–å¢å¼ºçŠ¶æ€
        initial_state = EnhancedMedicalWorkflowState(
            query=query,
            difficulty_level=None,
            recruited_experts=[],
            agent_responses=[],
            interaction_history=[],
            final_decision=None,
            confidence_score=None,
            processing_metadata=None,
            total_input_tokens=0,
            total_output_tokens=0,
            rag_results=[],
            knowledge_context=None,
            followup_queries=[],
            evidence_quality_score=None
        )
        
        # è¿è¡ŒRAGå¢å¼ºå·¥ä½œæµ
        start_time = time.time()
        final_state = self.workflow.invoke(initial_state)
        total_time = time.time() - start_time
        
        # æ„å»ºå¢å¼ºçš„è¿”å›ç»“æœ
        result = {
            "query_id": f"medrag_{int(time.time())}",
            "query": query.query_text,
            "difficulty_level": final_state["difficulty_level"].value,
            "final_decision": final_state["final_decision"],
            "confidence_score": final_state["confidence_score"],
            "evidence_quality_score": final_state["evidence_quality_score"],
            "expert_count": len(final_state["agent_responses"]),
            "sources_count": len(final_state["rag_results"][0].sources) if final_state["rag_results"] else 0,
            "followup_queries": final_state["followup_queries"],
            "token_usage": {
                "input_tokens": final_state["total_input_tokens"],
                "output_tokens": final_state["total_output_tokens"],
                "total_tokens": final_state["total_input_tokens"] + final_state["total_output_tokens"]
            },
            "processing_time": total_time,
            "rag_enhanced": True,
            "retrieval_info": {
                "retrieval_time": final_state["rag_results"][0].retrieval_time if final_state["rag_results"] else 0,
                "passages_retrieved": final_state["rag_results"][0].total_passages if final_state["rag_results"] else 0,
                "corpus_used": self.medrag_config.corpus_name,
                "retriever_used": self.medrag_config.retriever_name
            },
            "sources": final_state["rag_results"][0].sources if final_state["rag_results"] else [],
            "metadata": final_state["processing_metadata"]
        }
        
        print(f"\n[MedRAG-LangGraph] âœ¨ RAG å¢å¼ºå¤„ç†å®Œæˆ! è€—æ—¶: {total_time:.2f}s")
        print(f"[MedRAG-LangGraph] ğŸ“š æ£€ç´¢åˆ° {result['sources_count']} ä¸ªåŒ»å­¦çŸ¥è¯†æº")
        print(f"[MedRAG-LangGraph] ğŸ“Š è¯æ®è´¨é‡: {result['evidence_quality_score']:.2f}")
        print(f"[MedRAG-LangGraph] ğŸ¯ æœ€ç»ˆç½®ä¿¡åº¦: {result['confidence_score']:.2f}")
        
        return result

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # é…ç½® MedRAG
    medrag_config = MedRAGConfig(
        corpus_name="pubmed",
        retriever_name="medcpt", 
        retrieve_k=5,
        enable_rerank=True,
        enable_followup_query=True
    )
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    rag_system = MedRAGIntegratedSystem(
        model_name="gemini-2.5-flash-lite-preview-06-17",
        medrag_config=medrag_config
    )
    
    # æµ‹è¯•æŸ¥è¯¢
    test_query = MedicalQuery(
        query_text="A 65-year-old patient with diabetes presents with acute chest pain. Blood pressure is 180/100. What is the optimal emergency management approach?",
        query_type=QueryType.DIAGNOSIS
    )
    
    # å¤„ç†æŸ¥è¯¢
    result = asyncio.run(rag_system.process_medical_query(test_query))
    
    # è¾“å‡ºç»“æœ
    print("\n" + "="*80)
    print("ğŸ¥ MEDRAG-LANGGRAPH å¤„ç†ç»“æœ")
    print("="*80)
    print(f"æŸ¥è¯¢ID: {result['query_id']}")
    print(f"éš¾åº¦çº§åˆ«: {result['difficulty_level']}")
    print(f"ä¸“å®¶æ•°é‡: {result['expert_count']}")
    print(f"çŸ¥è¯†æºæ•°é‡: {result['sources_count']}")
    print(f"è¯æ®è´¨é‡: {result['evidence_quality_score']:.2f}")
    print(f"å¤„ç†æ—¶é—´: {result['processing_time']:.2f}ç§’")
    print(f"æ£€ç´¢è€—æ—¶: {result['retrieval_info']['retrieval_time']:.2f}ç§’")
    print(f"æœ€ç»ˆç½®ä¿¡åº¦: {result['confidence_score']:.2f}")
    print(f"Tokenä½¿ç”¨: {result['token_usage']['total_tokens']}")
    
    print(f"\nğŸ“š ä½¿ç”¨çš„çŸ¥è¯†åº“: {result['retrieval_info']['corpus_used']}")
    print(f"ğŸ” æ£€ç´¢å™¨: {result['retrieval_info']['retriever_used']}")
    print(f"ğŸ“„ æ£€ç´¢ç‰‡æ®µæ•°: {result['retrieval_info']['passages_retrieved']}")
    
    if result['followup_queries']:
        print(f"\nğŸ’¡ å»ºè®®çš„åç»­æŸ¥è¯¢:")
        for i, query in enumerate(result['followup_queries'], 1):
            print(f"   {i}. {query}")
    
    print(f"\nğŸ¯ æœ€ç»ˆ RAG å¢å¼ºå»ºè®®:")
    print("-" * 60)
    print(result['final_decision'])
    
    if result['sources']:
        print(f"\nğŸ“– å‚è€ƒæ¥æº:")
        for i, source in enumerate(result['sources'][:3], 1):  # æ˜¾ç¤ºå‰3ä¸ªæ¥æº
            print(f"   {i}. {source['title']} (è¯„åˆ†: {source.get('score', 'N/A')})")
    
    print("="*80)