"""
MediAgents-LangGraph å®ç°æ–¹æ¡ˆ
åŸºäºç°æœ‰æ¶æ„çš„ LangGraph é‡æ„ç¤ºä¾‹
"""

from typing import TypedDict, Annotated, List, Dict, Any, Optional
from typing_extensions import Literal
from langgraph.graph import StateGraph, END
import operator
import asyncio
from dataclasses import dataclass
from enum import Enum
import json
import time
from pydantic import BaseModel, Field

# å¯¼å…¥ç°æœ‰çš„å·¥å…·å‡½æ•°
from utils import Agent, setup_model, determine_difficulty

class DifficultyLevel(str, Enum):
    BASIC = "basic"
    INTERMEDIATE = "intermediate" 
    ADVANCED = "advanced"

class QueryType(str, Enum):
    DIAGNOSIS = "diagnosis"
    TREATMENT = "treatment"
    RESEARCH = "research"
    EDUCATION = "education"

@dataclass
class AgentResponse:
    agent_id: str
    role: str
    response: str
    confidence: float
    tokens_used: Dict[str, int]
    processing_time: float

@dataclass
class MedicalQuery:
    query_text: str
    query_type: QueryType = QueryType.DIAGNOSIS
    context: Optional[Dict[str, Any]] = None
    priority: int = 1

class MedicalWorkflowState(TypedDict):
    """LangGraph å·¥ä½œæµçŠ¶æ€å®šä¹‰"""
    # è¾“å…¥çŠ¶æ€
    query: MedicalQuery
    difficulty_level: Optional[DifficultyLevel]
    
    # å¤„ç†è¿‡ç¨‹çŠ¶æ€
    recruited_experts: Annotated[List[Dict], operator.add]
    agent_responses: Annotated[List[AgentResponse], operator.add]
    interaction_history: Annotated[List[Dict], operator.add]
    
    # è¾“å‡ºçŠ¶æ€
    final_decision: Optional[str]
    confidence_score: Optional[float]
    processing_metadata: Optional[Dict[str, Any]]
    
    # Token ä½¿ç”¨è·Ÿè¸ª
    total_input_tokens: Annotated[int, operator.add]
    total_output_tokens: Annotated[int, operator.add]

class MediAgentsLangGraph:
    """åŸºäº LangGraph çš„ MediAgents ç³»ç»Ÿ"""
    
    def __init__(self, model_name: str = "gemini-2.5-flash-lite-preview-06-17"):
        self.model_name = model_name
        self.setup_successful = setup_model(model_name)
        
        if not self.setup_successful:
            raise ValueError(f"Failed to setup model: {model_name}")
        
        # åˆ›å»ºå·¥ä½œæµå›¾
        self.workflow = self._create_workflow()
        
    def _create_workflow(self) -> StateGraph:
        """åˆ›å»º LangGraph å·¥ä½œæµ"""
        workflow = StateGraph(MedicalWorkflowState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("difficulty_assessor", self._assess_difficulty)
        workflow.add_node("expert_recruiter", self._recruit_experts)
        workflow.add_node("basic_processor", self._process_basic_workflow)
        workflow.add_node("intermediate_processor", self._process_intermediate_workflow)
        workflow.add_node("advanced_processor", self._process_advanced_workflow)
        workflow.add_node("response_synthesizer", self._synthesize_response)
        workflow.add_node("quality_validator", self._validate_quality)
        
        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("difficulty_assessor")
        
        # æ·»åŠ æ¡ä»¶è¾¹
        workflow.add_conditional_edges(
            "difficulty_assessor",
            self._route_by_difficulty,
            {
                "basic": "expert_recruiter",
                "intermediate": "expert_recruiter", 
                "advanced": "expert_recruiter"
            }
        )
        
        workflow.add_conditional_edges(
            "expert_recruiter",
            self._route_to_processor,
            {
                "basic": "basic_processor",
                "intermediate": "intermediate_processor",
                "advanced": "advanced_processor"
            }
        )
        
        # æ·»åŠ æ™®é€šè¾¹
        workflow.add_edge("basic_processor", "response_synthesizer")
        workflow.add_edge("intermediate_processor", "response_synthesizer") 
        workflow.add_edge("advanced_processor", "response_synthesizer")
        workflow.add_edge("response_synthesizer", "quality_validator")
        workflow.add_edge("quality_validator", END)
        
        return workflow.compile()
    
    def _assess_difficulty(self, state: MedicalWorkflowState) -> MedicalWorkflowState:
        """è¯„ä¼°æŸ¥è¯¢éš¾åº¦"""
        print("[LangGraph] ğŸ” è¯„ä¼°æŸ¥è¯¢éš¾åº¦...")
        
        query = state["query"]
        start_time = time.time()
        
        # ä½¿ç”¨ç°æœ‰çš„éš¾åº¦è¯„ä¼°å‡½æ•°
        difficulty_level, input_tokens, output_tokens = determine_difficulty(
            query.query_text, 
            "adaptive", 
            self.model_name
        )
        
        processing_time = time.time() - start_time
        
        print(f"[LangGraph] âœ… éš¾åº¦è¯„ä¼°å®Œæˆ: {difficulty_level} (è€—æ—¶: {processing_time:.2f}s)")
        
        return {
            **state,
            "difficulty_level": DifficultyLevel(difficulty_level),
            "total_input_tokens": input_tokens,
            "total_output_tokens": output_tokens,
            "processing_metadata": {
                "difficulty_assessment_time": processing_time
            }
        }
    
    def _recruit_experts(self, state: MedicalWorkflowState) -> MedicalWorkflowState:
        """æ‹›å‹Ÿä¸“å®¶å›¢é˜Ÿ"""
        print(f"[LangGraph] ğŸ‘¥ æ‹›å‹Ÿ {state['difficulty_level']} çº§åˆ«ä¸“å®¶å›¢é˜Ÿ...")
        
        difficulty = state["difficulty_level"]
        query = state["query"]
        
        # æ ¹æ®éš¾åº¦çº§åˆ«ç¡®å®šä¸“å®¶é…ç½®
        expert_configs = self._get_expert_configs(difficulty, query)
        
        recruited_experts = []
        for config in expert_configs:
            recruited_experts.append({
                "role": config["role"],
                "expertise": config["expertise"],
                "weight": config.get("weight", 1.0),
                "agent_id": f"{config['role']}_{len(recruited_experts)}"
            })
        
        print(f"[LangGraph] âœ… æˆåŠŸæ‹›å‹Ÿ {len(recruited_experts)} ä½ä¸“å®¶")
        
        return {
            **state,
            "recruited_experts": recruited_experts
        }
    
    def _get_expert_configs(self, difficulty: DifficultyLevel, query: MedicalQuery) -> List[Dict]:
        """æ ¹æ®éš¾åº¦å’ŒæŸ¥è¯¢ç±»å‹è·å–ä¸“å®¶é…ç½®"""
        base_configs = {
            DifficultyLevel.BASIC: [
                {"role": "General Practitioner", "expertise": "comprehensive medical knowledge and clinical experience"},
                {"role": "Specialist", "expertise": "specialized medical knowledge in relevant field"},
                {"role": "Clinical Researcher", "expertise": "evidence-based medicine and research methodology"}
            ],
            DifficultyLevel.INTERMEDIATE: [
                {"role": "Senior Clinician", "expertise": "advanced clinical decision-making"},
                {"role": "Medical Specialist", "expertise": "domain-specific expert knowledge"},
                {"role": "Research Physician", "expertise": "latest medical research and guidelines"},
                {"role": "Consultant", "expertise": "multi-disciplinary medical consultation"}
            ],
            DifficultyLevel.ADVANCED: [
                {"role": "Department Head", "expertise": "comprehensive medical leadership and expertise"},
                {"role": "Research Director", "expertise": "cutting-edge medical research and innovation"},
                {"role": "Multi-disciplinary Coordinator", "expertise": "coordinating complex medical teams"},
                {"role": "Ethics Consultant", "expertise": "medical ethics and complex decision-making"}
            ]
        }
        
        return base_configs.get(difficulty, base_configs[DifficultyLevel.BASIC])
    
    def _process_basic_workflow(self, state: MedicalWorkflowState) -> MedicalWorkflowState:
        """å¤„ç†åŸºç¡€çº§åˆ«æŸ¥è¯¢ - ç®€åŒ–çš„ä¸“å®¶ä»²è£æ¨¡å¼"""
        print("[LangGraph] ğŸ”„ æ‰§è¡ŒåŸºç¡€çº§åˆ«å¤„ç†æµç¨‹...")
        
        query = state["query"]
        experts = state["recruited_experts"]
        
        agent_responses = []
        total_input = total_output = 0
        
        # åˆ›å»ºä¸“å®¶æ™ºèƒ½ä½“å¹¶è·å–ç‹¬ç«‹æ„è§
        for expert in experts:
            agent = Agent(
                instruction=f"You are a {expert['role']} with expertise in {expert['expertise']}. Provide your professional medical opinion.",
                role=expert['role'],
                model_info=self.model_name
            )
            
            start_time = time.time()
            response_text = agent.chat(
                f"Medical Query: {query.query_text}\n\nProvide your professional analysis and recommendation (limit to 200 words):"
            )
            processing_time = time.time() - start_time
            
            agent_response = AgentResponse(
                agent_id=expert['agent_id'],
                role=expert['role'], 
                response=response_text,
                confidence=0.8,  # å¯ä»¥åç»­æ”¹è¿›ä¸ºåŠ¨æ€è®¡ç®—
                tokens_used={
                    "input": agent.total_input_tokens,
                    "output": agent.total_output_tokens
                },
                processing_time=processing_time
            )
            
            agent_responses.append(agent_response)
            total_input += agent.total_input_tokens
            total_output += agent.total_output_tokens
        
        print(f"[LangGraph] âœ… åŸºç¡€å¤„ç†å®Œæˆï¼Œæ”¶é›† {len(agent_responses)} ä¸ªä¸“å®¶æ„è§")
        
        return {
            **state,
            "agent_responses": agent_responses,
            "total_input_tokens": total_input,
            "total_output_tokens": total_output
        }
    
    def _process_intermediate_workflow(self, state: MedicalWorkflowState) -> MedicalWorkflowState:
        """å¤„ç†ä¸­çº§æŸ¥è¯¢ - ä¸“å®¶åä½œè®¨è®ºæ¨¡å¼"""
        print("[LangGraph] ğŸ”„ æ‰§è¡Œä¸­çº§å¤„ç†æµç¨‹...")
        
        query = state["query"]
        experts = state["recruited_experts"]
        
        # ç¬¬ä¸€è½®ï¼šæ”¶é›†åˆå§‹æ„è§
        initial_responses = self._collect_initial_opinions(query, experts)
        
        # ç¬¬äºŒè½®ï¼šåŸºäºå…¶ä»–ä¸“å®¶æ„è§è¿›è¡Œè®¨è®º
        discussion_responses = self._conduct_expert_discussion(query, experts, initial_responses)
        
        # åˆå¹¶æ‰€æœ‰å“åº”
        all_responses = initial_responses + discussion_responses
        
        # è®¡ç®—æ€»tokenä½¿ç”¨é‡
        total_input = sum(r.tokens_used["input"] for r in all_responses)
        total_output = sum(r.tokens_used["output"] for r in all_responses)
        
        print(f"[LangGraph] âœ… ä¸­çº§å¤„ç†å®Œæˆï¼Œè¿›è¡Œäº† {len(all_responses)} è½®ä¸“å®¶äº¤äº’")
        
        return {
            **state,
            "agent_responses": all_responses,
            "total_input_tokens": total_input,
            "total_output_tokens": total_output,
            "interaction_history": [
                {"round": 1, "type": "initial_opinions", "count": len(initial_responses)},
                {"round": 2, "type": "discussion", "count": len(discussion_responses)}
            ]
        }
    
    def _process_advanced_workflow(self, state: MedicalWorkflowState) -> MedicalWorkflowState:
        """å¤„ç†é«˜çº§æŸ¥è¯¢ - å¤šå­¦ç§‘å›¢é˜Ÿåè°ƒæ¨¡å¼"""
        print("[LangGraph] ğŸ”„ æ‰§è¡Œé«˜çº§å¤„ç†æµç¨‹...")
        
        query = state["query"]
        experts = state["recruited_experts"]
        
        # é«˜çº§æµç¨‹ï¼šå¤šè½®è®¨è®º + åè°ƒå‘˜ç»¼åˆ
        responses = []
        
        # ç¬¬ä¸€é˜¶æ®µï¼šä¸“ä¸šåˆ†æ
        specialist_responses = self._specialist_analysis_phase(query, experts)
        responses.extend(specialist_responses)
        
        # ç¬¬äºŒé˜¶æ®µï¼šè·¨å­¦ç§‘è®¨è®º
        interdisciplinary_responses = self._interdisciplinary_discussion_phase(
            query, experts, specialist_responses
        )
        responses.extend(interdisciplinary_responses)
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šåè°ƒå‘˜ç»¼åˆ
        coordinator_response = self._coordinator_synthesis_phase(
            query, experts, responses
        )
        responses.append(coordinator_response)
        
        # è®¡ç®—æ€»tokenä½¿ç”¨é‡
        total_input = sum(r.tokens_used["input"] for r in responses)
        total_output = sum(r.tokens_used["output"] for r in responses)
        
        print(f"[LangGraph] âœ… é«˜çº§å¤„ç†å®Œæˆï¼Œå®Œæˆ 3 é˜¶æ®µå…± {len(responses)} æ¬¡ä¸“å®¶äº¤äº’")
        
        return {
            **state,
            "agent_responses": responses,
            "total_input_tokens": total_input,
            "total_output_tokens": total_output,
            "interaction_history": [
                {"phase": 1, "type": "specialist_analysis", "count": len(specialist_responses)},
                {"phase": 2, "type": "interdisciplinary_discussion", "count": len(interdisciplinary_responses)},
                {"phase": 3, "type": "coordinator_synthesis", "count": 1}
            ]
        }
    
    def _synthesize_response(self, state: MedicalWorkflowState) -> MedicalWorkflowState:
        """ç»¼åˆæœ€ç»ˆå“åº”"""
        print("[LangGraph] ğŸ¯ ç»¼åˆæœ€ç»ˆåŒ»å­¦å»ºè®®...")
        
        query = state["query"]
        agent_responses = state["agent_responses"]
        difficulty = state["difficulty_level"]
        
        # åˆ›å»ºç»¼åˆæ™ºèƒ½ä½“
        synthesizer = Agent(
            instruction="You are a senior medical consultant responsible for synthesizing multiple expert opinions into a comprehensive, actionable medical recommendation.",
            role="Medical Synthesizer",
            model_info=self.model_name
        )
        
        # æ„å»ºç»¼åˆæç¤º
        expert_opinions = "\n\n".join([
            f"**{resp.role} Opinion:**\n{resp.response}"
            for resp in agent_responses
        ])
        
        synthesis_prompt = f"""
        Medical Query: {query.query_text}
        
        Expert Opinions to Synthesize:
        {expert_opinions}
        
        Please provide a comprehensive, well-structured medical recommendation that:
        1. Synthesizes the key insights from all expert opinions
        2. Addresses any conflicting viewpoints with evidence-based reasoning
        3. Provides clear, actionable recommendations
        4. Includes appropriate caveats and follow-up suggestions
        
        Structure your response with clear sections and limit to 400 words.
        """
        
        start_time = time.time()
        final_response = synthesizer.chat(synthesis_prompt)
        synthesis_time = time.time() - start_time
        
        # è®¡ç®—ç»¼åˆç½®ä¿¡åº¦ (ç®€åŒ–ç‰ˆæœ¬)
        avg_confidence = sum(r.confidence for r in agent_responses) / len(agent_responses)
        confidence_score = min(avg_confidence * 0.95, 0.95)  # è½»å¾®é™ä½ä»¥åæ˜ ç»¼åˆçš„ä¸ç¡®å®šæ€§
        
        print(f"[LangGraph] âœ… æœ€ç»ˆå»ºè®®ç»¼åˆå®Œæˆ (ç½®ä¿¡åº¦: {confidence_score:.2f})")
        
        return {
            **state,
            "final_decision": final_response,
            "confidence_score": confidence_score,
            "total_input_tokens": synthesizer.total_input_tokens,
            "total_output_tokens": synthesizer.total_output_tokens,
            "processing_metadata": {
                **state.get("processing_metadata", {}),
                "synthesis_time": synthesis_time,
                "total_expert_responses": len(agent_responses)
            }
        }
    
    def _validate_quality(self, state: MedicalWorkflowState) -> MedicalWorkflowState:
        """è´¨é‡éªŒè¯å’Œæœ€ç»ˆæ£€æŸ¥"""
        print("[LangGraph] âœ… æ‰§è¡Œè´¨é‡éªŒè¯...")
        
        final_decision = state["final_decision"]
        confidence = state["confidence_score"]
        
        # ç®€å•çš„è´¨é‡æ£€æŸ¥
        quality_metrics = {
            "response_length": len(final_decision.split()),
            "confidence_score": confidence,
            "expert_count": len(state["agent_responses"]),
            "processing_complete": final_decision is not None
        }
        
        # å¯ä»¥æ·»åŠ æ›´å¤æ‚çš„è´¨é‡éªŒè¯é€»è¾‘
        quality_passed = (
            quality_metrics["response_length"] > 50 and
            quality_metrics["confidence_score"] > 0.5 and
            quality_metrics["processing_complete"]
        )
        
        print(f"[LangGraph] ğŸ“Š è´¨é‡éªŒè¯å®Œæˆ - {'é€šè¿‡' if quality_passed else 'éœ€è¦æ”¹è¿›'}")
        
        return {
            **state,
            "processing_metadata": {
                **state.get("processing_metadata", {}),
                "quality_metrics": quality_metrics,
                "quality_passed": quality_passed
            }
        }
    
    # è·¯ç”±å‡½æ•°
    def _route_by_difficulty(self, state: MedicalWorkflowState) -> Literal["basic", "intermediate", "advanced"]:
        """æ ¹æ®éš¾åº¦è·¯ç”±åˆ°ä¸åŒå¤„ç†æµç¨‹"""
        difficulty = state["difficulty_level"]
        print(f"[LangGraph] ğŸ”€ è·¯ç”±åˆ° {difficulty.value} å¤„ç†æµç¨‹")
        return difficulty.value
    
    def _route_to_processor(self, state: MedicalWorkflowState) -> Literal["basic", "intermediate", "advanced"]:
        """è·¯ç”±åˆ°å…·ä½“çš„å¤„ç†å™¨"""
        return state["difficulty_level"].value
    
    # è¾…åŠ©æ–¹æ³•
    def _collect_initial_opinions(self, query: MedicalQuery, experts: List[Dict]) -> List[AgentResponse]:
        """æ”¶é›†ä¸“å®¶åˆå§‹æ„è§"""
        responses = []
        for expert in experts:
            agent = Agent(
                instruction=f"You are a {expert['role']} with expertise in {expert['expertise']}.",
                role=expert['role'],
                model_info=self.model_name
            )
            
            start_time = time.time()
            response_text = agent.chat(
                f"Medical Query: {query.query_text}\n\nProvide your initial professional analysis (limit to 150 words):"
            )
            processing_time = time.time() - start_time
            
            responses.append(AgentResponse(
                agent_id=expert['agent_id'],
                role=expert['role'],
                response=response_text,
                confidence=0.75,
                tokens_used={"input": agent.total_input_tokens, "output": agent.total_output_tokens},
                processing_time=processing_time
            ))
        
        return responses
    
    def _conduct_expert_discussion(self, query: MedicalQuery, experts: List[Dict], initial_responses: List[AgentResponse]) -> List[AgentResponse]:
        """ä¸“å®¶è®¨è®ºé˜¶æ®µ"""
        discussion_responses = []
        
        # æ„å»ºå…¶ä»–ä¸“å®¶æ„è§çš„æ‘˜è¦
        other_opinions = "\n".join([
            f"{resp.role}: {resp.response[:100]}..."
            for resp in initial_responses
        ])
        
        for i, expert in enumerate(experts):
            agent = Agent(
                instruction=f"You are a {expert['role']} participating in a medical consultation discussion.",
                role=expert['role'],
                model_info=self.model_name
            )
            
            discussion_prompt = f"""
            Medical Query: {query.query_text}
            
            Other Expert Opinions:
            {other_opinions}
            
            Based on the other experts' initial opinions, provide your refined analysis and any additional insights (limit to 100 words):
            """
            
            start_time = time.time()
            response_text = agent.chat(discussion_prompt)
            processing_time = time.time() - start_time
            
            discussion_responses.append(AgentResponse(
                agent_id=f"{expert['agent_id']}_discussion",
                role=f"{expert['role']} (Discussion)",
                response=response_text,
                confidence=0.8,
                tokens_used={"input": agent.total_input_tokens, "output": agent.total_output_tokens},
                processing_time=processing_time
            ))
        
        return discussion_responses
    
    def _specialist_analysis_phase(self, query: MedicalQuery, experts: List[Dict]) -> List[AgentResponse]:
        """ä¸“å®¶åˆ†æé˜¶æ®µ"""
        return self._collect_initial_opinions(query, experts)
    
    def _interdisciplinary_discussion_phase(self, query: MedicalQuery, experts: List[Dict], previous_responses: List[AgentResponse]) -> List[AgentResponse]:
        """è·¨å­¦ç§‘è®¨è®ºé˜¶æ®µ"""
        return self._conduct_expert_discussion(query, experts, previous_responses)
    
    def _coordinator_synthesis_phase(self, query: MedicalQuery, experts: List[Dict], all_responses: List[AgentResponse]) -> AgentResponse:
        """åè°ƒå‘˜ç»¼åˆé˜¶æ®µ"""
        coordinator = Agent(
            instruction="You are a senior medical coordinator responsible for synthesizing multidisciplinary team discussions.",
            role="MDT Coordinator",
            model_info=self.model_name
        )
        
        all_opinions = "\n\n".join([
            f"**{resp.role}:**\n{resp.response}"
            for resp in all_responses
        ])
        
        coordinator_prompt = f"""
        Medical Query: {query.query_text}
        
        Multidisciplinary Team Analysis:
        {all_opinions}
        
        As the MDT coordinator, provide a final coordinated recommendation that integrates all perspectives (limit to 200 words):
        """
        
        start_time = time.time()
        response_text = coordinator.chat(coordinator_prompt)
        processing_time = time.time() - start_time
        
        return AgentResponse(
            agent_id="mdt_coordinator",
            role="MDT Coordinator",
            response=response_text,
            confidence=0.9,
            tokens_used={"input": coordinator.total_input_tokens, "output": coordinator.total_output_tokens},
            processing_time=processing_time
        )
    
    async def process_query(self, query: MedicalQuery) -> Dict[str, Any]:
        """å¤„ç†åŒ»å­¦æŸ¥è¯¢çš„ä¸»è¦å…¥å£ç‚¹"""
        print(f"\n[LangGraph] ğŸš€ å¼€å§‹å¤„ç†åŒ»å­¦æŸ¥è¯¢: {query.query_text[:50]}...")
        
        # åˆå§‹åŒ–çŠ¶æ€
        initial_state = MedicalWorkflowState(
            query=query,
            difficulty_level=None,
            recruited_experts=[],
            agent_responses=[],
            interaction_history=[],
            final_decision=None,
            confidence_score=None,
            processing_metadata=None,
            total_input_tokens=0,
            total_output_tokens=0
        )
        
        # è¿è¡Œå·¥ä½œæµ
        start_time = time.time()
        final_state = self.workflow.invoke(initial_state)
        total_time = time.time() - start_time
        
        # æ„å»ºè¿”å›ç»“æœ
        result = {
            "query_id": f"medq_{int(time.time())}",
            "query": query.query_text,
            "difficulty_level": final_state["difficulty_level"].value,
            "final_decision": final_state["final_decision"],
            "confidence_score": final_state["confidence_score"],
            "expert_count": len(final_state["agent_responses"]),
            "token_usage": {
                "input_tokens": final_state["total_input_tokens"],
                "output_tokens": final_state["total_output_tokens"],
                "total_tokens": final_state["total_input_tokens"] + final_state["total_output_tokens"]
            },
            "processing_time": total_time,
            "metadata": final_state["processing_metadata"]
        }
        
        print(f"\n[LangGraph] âœ¨ æŸ¥è¯¢å¤„ç†å®Œæˆ! è€—æ—¶: {total_time:.2f}s")
        print(f"[LangGraph] ğŸ“Š Tokenä½¿ç”¨: {result['token_usage']['total_tokens']}")
        print(f"[LangGraph] ğŸ¯ ç½®ä¿¡åº¦: {result['confidence_score']:.2f}")
        
        return result

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    mediagents = MediAgentsLangGraph()
    
    # æµ‹è¯•æŸ¥è¯¢
    test_query = MedicalQuery(
        query_text="A 45-year-old patient presents with chest pain and shortness of breath. What should be the immediate diagnostic approach?",
        query_type=QueryType.DIAGNOSIS
    )
    
    # å¤„ç†æŸ¥è¯¢
    result = asyncio.run(mediagents.process_query(test_query))
    
    # è¾“å‡ºç»“æœ
    print("\n" + "="*60)
    print("ğŸ¥ MEDLANGRAPH å¤„ç†ç»“æœ")
    print("="*60)
    print(f"æŸ¥è¯¢ID: {result['query_id']}")
    print(f"éš¾åº¦çº§åˆ«: {result['difficulty_level']}")
    print(f"ä¸“å®¶æ•°é‡: {result['expert_count']}")
    print(f"å¤„ç†æ—¶é—´: {result['processing_time']:.2f}ç§’")
    print(f"ç½®ä¿¡åº¦: {result['confidence_score']:.2f}")
    print(f"Tokenä½¿ç”¨: {result['token_usage']['total_tokens']}")
    print("\nğŸ’¡ æœ€ç»ˆå»ºè®®:")
    print("-" * 40)
    print(result['final_decision'])
    print("="*60)