# MediAgents-LangGraph æ¶æ„ä¼˜åŒ–æŒ‡å—

åŸºäºä¹å¤§æ­¥éª¤æ¡†æ¶çš„ AI Agent ç³»ç»Ÿå‡çº§æ–¹æ¡ˆ

## ğŸ“‹ é¡¹ç›®ç°çŠ¶åˆ†æ

### å½“å‰æ¶æ„ç‰¹ç‚¹
- **ç®€å•æµç¨‹æ¶æ„**: åŸºäºéš¾åº¦åˆ†çº§çš„çº¿æ€§å¤„ç†ï¼ˆbasic â†’ intermediate â†’ advancedï¼‰
- **å•ä¸€æ¨¡å‹ä¾èµ–**: ä¾èµ– Gemini/OpenAI å•ä¸€æ¨¡å‹å¤„ç†æ‰€æœ‰ä»»åŠ¡
- **æœ‰é™åä½œæœºåˆ¶**: åŸºç¡€çš„ Agent ç±»å’Œ Group ç±»ï¼Œç¼ºä¹å¤æ‚äº¤äº’
- **æˆæœ¬æ§åˆ¶å¯¼å‘**: å¼ºè°ƒ token ä½¿ç”¨æ•ˆç‡ï¼Œä½†é™åˆ¶äº†åŠŸèƒ½æ‰©å±•

### æ ¸å¿ƒç—›ç‚¹è¯†åˆ«
1. **æ¶æ„åƒµåŒ–**: ç¼ºä¹çµæ´»çš„å·¥ä½œæµç¼–æ’èƒ½åŠ›
2. **è®°å¿†ç¼ºå¤±**: æ— é•¿æœŸè®°å¿†å’Œä¸Šä¸‹æ–‡ç®¡ç†
3. **å·¥å…·å—é™**: ç¼ºä¹å¤–éƒ¨å·¥å…·é›†æˆå’Œ RAG èƒ½åŠ›
4. **å•ä¸€äº¤äº’**: ä»…æ”¯æŒå‘½ä»¤è¡Œï¼Œç¼ºä¹å‰ç«¯ç•Œé¢
5. **è¯„ä¼°å•ä¸€**: ä»…åŸºäºå‡†ç¡®æ€§ï¼Œç¼ºä¹å¤šç»´åº¦è¯„ä¼°

---

## ğŸ¯ ä¹å¤§æ­¥éª¤ä¼˜åŒ–æ–¹æ¡ˆ

### Step 1: æ˜ç¡®ç›®æ ‡ä¸åœºæ™¯é‡æ–°å®šä¹‰

#### ä¼˜åŒ–ç›®æ ‡
- **ä»å•ä¸€é—®ç­”** â†’ **å¤šæ¨¡æ€åŒ»ç–—åŠ©æ‰‹**
- **ä»æˆæœ¬ä¼˜å…ˆ** â†’ **æ€§èƒ½ä¸æˆæœ¬å¹³è¡¡**
- **ä»è¯„ä¼°å·¥å…·** â†’ **ç”Ÿäº§å°±ç»ªçš„åŒ»ç–— AI ç³»ç»Ÿ**

#### æ ¸å¿ƒåœºæ™¯æ‰©å±•
```python
# æ–°å¢åº”ç”¨åœºæ™¯
SCENARIOS = {
    "diagnostic_support": "è¯Šæ–­æ”¯æŒå’Œå»ºè®®",
    "treatment_planning": "æ²»ç–—æ–¹æ¡ˆåˆ¶å®š", 
    "literature_review": "æ–‡çŒ®ç»¼è¿°å’Œç ”ç©¶",
    "medical_education": "åŒ»å­¦æ•™è‚²å’ŒåŸ¹è®­",
    "clinical_decision": "ä¸´åºŠå†³ç­–æ”¯æŒ"
}
```

#### ç”¨æˆ·ç¾¤ä½“
- **åŒ»å­¦ç”Ÿ**: å­¦ä¹ è¾…åŠ©å’Œè€ƒè¯•å‡†å¤‡
- **ä¸´åºŠåŒ»ç”Ÿ**: è¯Šæ–­æ”¯æŒå’Œå†³ç­–è¾…åŠ©  
- **ç ”ç©¶äººå‘˜**: æ–‡çŒ®åˆ†æå’Œç ”ç©¶æ”¯æŒ
- **æ‚£è€…**: å¥åº·å’¨è¯¢å’Œä¿¡æ¯è·å–

### Step 2: è§„èŒƒè¾“å…¥è¾“å‡ºæ¶æ„

#### è¾“å…¥æ ‡å‡†åŒ–
```python
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
from enum import Enum

class QueryType(str, Enum):
    DIAGNOSIS = "diagnosis"
    TREATMENT = "treatment" 
    RESEARCH = "research"
    EDUCATION = "education"

class MedicalQuery(BaseModel):
    query_text: str = Field(..., min_length=10, max_length=2000)
    query_type: QueryType
    context: Optional[Dict[str, Any]] = None
    patient_info: Optional[Dict[str, str]] = None
    priority: int = Field(default=1, ge=1, le=5)
    require_sources: bool = Field(default=True)
    max_response_length: int = Field(default=500, ge=100, le=1000)

class MedicalContext(BaseModel):
    symptoms: Optional[List[str]] = None
    medical_history: Optional[List[str]] = None
    current_medications: Optional[List[str]] = None
    test_results: Optional[Dict[str, Any]] = None
```

#### è¾“å‡ºæ ‡å‡†åŒ–
```python
class MedicalResponse(BaseModel):
    response_id: str
    query_id: str
    primary_response: str
    confidence_score: float = Field(ge=0.0, le=1.0)
    supporting_evidence: List[Dict[str, str]]
    alternative_perspectives: Optional[List[str]] = None
    follow_up_questions: Optional[List[str]] = None
    sources: List[Dict[str, str]]
    processing_metadata: ProcessingMetadata

class ProcessingMetadata(BaseModel):
    agents_involved: List[str]
    processing_time: float
    difficulty_level: str
    token_usage: TokenUsage
    workflow_path: List[str]
```

### Step 3: åŸºäº LangGraph çš„æç¤ºè¯å·¥ç¨‹

#### Agent è§’è‰²å®šä¹‰
```python
AGENT_PROMPTS = {
    "diagnostic_specialist": {
        "role": "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„è¯Šæ–­ä¸“å®¶",
        "personality": "ä¸¥è°¨ã€ç»†è‡´ã€åŸºäºå¾ªè¯åŒ»å­¦",
        "specialization": "ç—‡çŠ¶åˆ†æã€é‰´åˆ«è¯Šæ–­ã€è¯Šæ–­å»ºè®®",
        "output_format": "ç»“æ„åŒ–è¯Šæ–­æŠ¥å‘Š"
    },
    
    "treatment_planner": {
        "role": "ä½ æ˜¯ä¸€ä½æ²»ç–—æ–¹æ¡ˆä¸“å®¶", 
        "personality": "å®ç”¨ã€å…¨é¢ã€è€ƒè™‘æ‚£è€…ä¸ªä½“å·®å¼‚",
        "specialization": "æ²»ç–—æ–¹æ¡ˆåˆ¶å®šã€è¯ç‰©é€‰æ‹©ã€ç–—æ•ˆè¯„ä¼°",
        "output_format": "åˆ†å±‚æ²»ç–—å»ºè®®"
    },
    
    "research_analyst": {
        "role": "ä½ æ˜¯ä¸€ä½åŒ»å­¦ç ”ç©¶åˆ†æå¸ˆ",
        "personality": "å®¢è§‚ã€æ‰¹åˆ¤æ€§æ€ç»´ã€å¾ªè¯å¯¼å‘",
        "specialization": "æ–‡çŒ®æ£€ç´¢ã€è¯æ®è¯„ä¼°ã€ç ”ç©¶ç»¼è¿°",
        "output_format": "å¾ªè¯åŒ»å­¦æŠ¥å‘Š"
    }
}
```

#### åŠ¨æ€æç¤ºè¯ç”Ÿæˆ
```python
def generate_context_aware_prompt(agent_type: str, query: MedicalQuery, context: Dict) -> str:
    base_prompt = AGENT_PROMPTS[agent_type]
    
    # æ ¹æ®æŸ¥è¯¢ç±»å‹å’Œä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´
    if query.query_type == QueryType.DIAGNOSIS:
        return f"""
        {base_prompt['role']}ï¼Œ{base_prompt['personality']}ã€‚
        
        æ‚£è€…å’¨è¯¢: {query.query_text}
        
        å·²çŸ¥ä¿¡æ¯:
        - ç—‡çŠ¶: {context.get('symptoms', 'æœªæä¾›')}
        - ç—…å²: {context.get('medical_history', 'æœªæä¾›')}
        
        è¯·æä¾›:
        1. å¯èƒ½è¯Šæ–­ï¼ˆæŒ‰æ¦‚ç‡æ’åºï¼‰
        2. è¿›ä¸€æ­¥æ£€æŸ¥å»ºè®®
        3. é‰´åˆ«è¯Šæ–­è¦ç‚¹
        
        è¾“å‡ºæ ¼å¼: {base_prompt['output_format']}
        å­—æ•°é™åˆ¶: {query.max_response_length} å­—ä»¥å†…
        """
```

### Step 4: LangGraph å·¥ä½œæµä¸å·¥å…·é›†æˆ

#### æ ¸å¿ƒå·¥ä½œæµè®¾è®¡
```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated
import operator

class MedicalWorkflowState(TypedDict):
    query: MedicalQuery
    context: MedicalContext
    difficulty_assessment: str
    agent_responses: Annotated[List[Dict], operator.add]
    final_synthesis: str
    confidence_scores: Annotated[List[float], operator.add]
    sources: Annotated[List[Dict], operator.add]

def create_medical_workflow() -> StateGraph:
    workflow = StateGraph(MedicalWorkflowState)
    
    # èŠ‚ç‚¹å®šä¹‰
    workflow.add_node("difficulty_assessor", assess_query_difficulty)
    workflow.add_node("rag_retriever", retrieve_relevant_knowledge)
    workflow.add_node("specialist_panel", convene_specialist_panel)
    workflow.add_node("evidence_evaluator", evaluate_evidence_quality)
    workflow.add_node("response_synthesizer", synthesize_final_response)
    workflow.add_node("quality_checker", perform_quality_check)
    
    # æ¡ä»¶è·¯ç”±
    workflow.add_conditional_edges(
        "difficulty_assessor",
        route_by_difficulty,
        {
            "basic": "specialist_panel",
            "intermediate": "rag_retriever", 
            "advanced": "evidence_evaluator"
        }
    )
    
    return workflow.compile()
```

#### RAG å·¥å…·é›†æˆ
```python
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.document_loaders import PubMedLoader

class MedicalRAGSystem:
    def __init__(self):
        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = Chroma(
            collection_name="medical_knowledge",
            embedding_function=self.embeddings
        )
        self.pubmed_loader = PubMedLoader()
    
    async def retrieve_context(self, query: str, k: int = 5) -> List[Document]:
        # æ··åˆæ£€ç´¢ï¼šå‘é‡ç›¸ä¼¼åº¦ + å…³é”®è¯åŒ¹é…
        vector_results = self.vectorstore.similarity_search(query, k=k)
        
        # å®æ—¶ PubMed æ£€ç´¢
        recent_papers = await self.pubmed_loader.load_recent_papers(
            query=query, 
            days_back=30,
            max_results=3
        )
        
        return vector_results + recent_papers
```

### Step 5: å¤š Agent åä½œæ¶æ„

#### ä¸“ä¸šåŒ– Agent è®¾è®¡
```python
class SpecializedMedicalAgent:
    def __init__(self, specialty: str, model_config: dict):
        self.specialty = specialty
        self.model = self.initialize_model(model_config)
        self.memory = ConversationBufferMemory()
        self.tools = self.get_specialty_tools()
    
    def get_specialty_tools(self) -> List[Tool]:
        specialty_tools = {
            "diagnostic": [
                DifferentialDiagnosisTool(),
                SymptomAnalysisTool(),
                TestRecommendationTool()
            ],
            "treatment": [
                DrugInteractionTool(), 
                GuidelineSearchTool(),
                DosageCalculatorTool()
            ],
            "research": [
                PubMedSearchTool(),
                ClinicalTrialsTool(),
                EvidenceGradingTool()
            ]
        }
        return specialty_tools.get(self.specialty, [])

class MedicalTeamCoordinator:
    def __init__(self):
        self.agents = {
            "diagnostician": SpecializedMedicalAgent("diagnostic", GEMINI_CONFIG),
            "clinician": SpecializedMedicalAgent("treatment", GEMINI_CONFIG), 
            "researcher": SpecializedMedicalAgent("research", GEMINI_CONFIG),
            "coordinator": CoordinatorAgent(GPT4_CONFIG)
        }
    
    async def process_query(self, query: MedicalQuery) -> MedicalResponse:
        # å¹¶è¡Œä¸“å®¶å’¨è¯¢
        expert_responses = await asyncio.gather(*[
            agent.process(query) for agent in self.agents.values()
        ])
        
        # åè°ƒå‘˜ç»¼åˆåˆ¤æ–­
        final_response = await self.agents["coordinator"].synthesize(
            query, expert_responses
        )
        
        return final_response
```

### Step 6: è®°å¿†ä¸ä¸Šä¸‹æ–‡ç®¡ç†

#### åˆ†å±‚è®°å¿†æ¶æ„
```python
from langchain.memory import ConversationSummaryBufferMemory
from langchain.vectorstores import FAISS
import redis

class MedicalMemorySystem:
    def __init__(self):
        # çŸ­æœŸè®°å¿†ï¼šå½“å‰å¯¹è¯
        self.short_term = ConversationSummaryBufferMemory(
            max_token_limit=1000,
            return_messages=True
        )
        
        # ä¸­æœŸè®°å¿†ï¼šä¼šè¯æ‘˜è¦
        self.mid_term = redis.Redis(host='localhost', port=6379, db=0)
        
        # é•¿æœŸè®°å¿†ï¼šçŸ¥è¯†å‘é‡åº“
        self.long_term = FAISS.from_documents(
            documents=[],  # é¢„åŠ è½½åŒ»å­¦çŸ¥è¯†åº“
            embedding=OpenAIEmbeddings()
        )
    
    async def store_interaction(self, query: MedicalQuery, response: MedicalResponse):
        # çŸ­æœŸè®°å¿†å­˜å‚¨
        self.short_term.save_context(
            {"input": query.query_text},
            {"output": response.primary_response}
        )
        
        # ä¸­æœŸè®°å¿†ï¼šå­˜å‚¨ä¼šè¯æ‘˜è¦
        session_id = self.get_session_id()
        summary = await self.generate_session_summary()
        self.mid_term.setex(f"session:{session_id}", 3600*24*7, summary)
        
        # é•¿æœŸè®°å¿†ï¼šæ›´æ–°çŸ¥è¯†åº“
        if response.confidence_score > 0.8:
            await self.update_knowledge_base(query, response)
```

### Step 7: å¤šæ¨¡æ€èƒ½åŠ›æ‰©å±•

#### å¤šæ¨¡æ€è¾“å…¥å¤„ç†
```python
from langchain.document_loaders import ImageLoader
from langchain.schema import Document
import speech_recognition as sr
from PIL import Image

class MultimodalProcessor:
    def __init__(self):
        self.speech_recognizer = sr.Recognizer()
        self.image_analyzer = GPT4VisionAnalyzer()
    
    async def process_audio_query(self, audio_file: str) -> str:
        """è¯­éŸ³è½¬æ–‡å­—"""
        with sr.AudioFile(audio_file) as source:
            audio = self.speech_recognizer.record(source)
        return self.speech_recognizer.recognize_google(audio, language='zh-CN')
    
    async def analyze_medical_image(self, image_path: str, query: str) -> Dict:
        """åŒ»å­¦å½±åƒåˆ†æ"""
        image_analysis = await self.image_analyzer.analyze(
            image_path=image_path,
            prompt=f"ä½œä¸ºåŒ»å­¦å½±åƒä¸“å®¶ï¼Œåˆ†æè¿™å¼ åŒ»å­¦å›¾åƒå¹¶å›ç­”ï¼š{query}"
        )
        
        return {
            "image_description": image_analysis.description,
            "medical_findings": image_analysis.findings,
            "recommendations": image_analysis.recommendations
        }
```

### Step 8: è¾“å‡ºæ ¼å¼ä¸å‰ç«¯é›†æˆ

#### Streamlit å‰ç«¯ç•Œé¢
```python
import streamlit as st
import asyncio
from typing import Optional

class MedicalChatbot:
    def __init__(self):
        self.workflow = create_medical_workflow()
        self.memory = MedicalMemorySystem()
    
    def render_interface(self):
        st.set_page_config(
            page_title="MediAgents Pro",
            page_icon="ğŸ¥",
            layout="wide"
        )
        
        # ä¾§è¾¹æ é…ç½®
        with st.sidebar:
            st.header("é…ç½®é€‰é¡¹")
            query_type = st.selectbox(
                "å’¨è¯¢ç±»å‹", 
                ["diagnosis", "treatment", "research", "education"]
            )
            
            confidence_threshold = st.slider(
                "ç½®ä¿¡åº¦é˜ˆå€¼", 0.0, 1.0, 0.7, 0.1
            )
            
            include_sources = st.checkbox("åŒ…å«å‚è€ƒæ¥æº", True)
        
        # ä¸»ç•Œé¢
        st.title("ğŸ¥ MediAgents Pro - æ™ºèƒ½åŒ»å­¦åŠ©æ‰‹")
        
        # æ–‡ä»¶ä¸Šä¼ 
        uploaded_files = st.file_uploader(
            "ä¸Šä¼ åŒ»å­¦å›¾åƒæˆ–æŠ¥å‘Š",
            type=['jpg', 'png', 'pdf', 'txt'],
            accept_multiple_files=True
        )
        
        # å¯¹è¯ç•Œé¢
        if "messages" not in st.session_state:
            st.session_state.messages = []
        
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
        
        # ç”¨æˆ·è¾“å…¥
        if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„åŒ»å­¦å’¨è¯¢..."):
            st.session_state.messages.append({
                "role": "user", 
                "content": prompt
            })
            
            with st.chat_message("user"):
                st.markdown(prompt)
            
            # AI å›å¤
            with st.chat_message("assistant"):
                response = await self.process_query(
                    prompt, query_type, uploaded_files
                )
                st.markdown(response.primary_response)
                
                # æ˜¾ç¤ºç½®ä¿¡åº¦å’Œæ¥æº
                if response.confidence_score < confidence_threshold:
                    st.warning(f"âš ï¸ ç½®ä¿¡åº¦è¾ƒä½ ({response.confidence_score:.2f})")
                
                if include_sources and response.sources:
                    with st.expander("å‚è€ƒæ¥æº"):
                        for source in response.sources:
                            st.write(f"- {source['title']}: {source['url']}")

# è¿è¡Œåº”ç”¨
if __name__ == "__main__":
    chatbot = MedicalChatbot()
    chatbot.render_interface()
```

### Step 9: API éƒ¨ç½²ä¸ç›‘æ§

#### FastAPI æœåŠ¡éƒ¨ç½²
```python
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

app = FastAPI(
    title="MediAgents API",
    description="é«˜æ€§èƒ½åŒ»å­¦ AI åŠ©æ‰‹ API",
    version="2.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/api/v2/medical-query", response_model=MedicalResponse)
async def process_medical_query(
    query: MedicalQuery,
    files: Optional[List[UploadFile]] = File(None)
):
    try:
        # å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶
        file_contexts = []
        if files:
            for file in files:
                file_context = await process_uploaded_file(file)
                file_contexts.append(file_context)
        
        # å¤„ç†æŸ¥è¯¢
        workflow = create_medical_workflow()
        result = await workflow.ainvoke({
            "query": query,
            "context": MedicalContext(file_contexts=file_contexts)
        })
        
        return result["final_response"]
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v2/health")
async def health_check():
    return {"status": "healthy", "version": "2.0.0"}

# æ€§èƒ½ç›‘æ§
@app.middleware("http")
async def monitor_performance(request, call_next):
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    
    # è®°å½•æ€§èƒ½æŒ‡æ ‡
    logger.info(f"Request processed in {process_time:.2f}s")
    
    return response
```

---

## ğŸ› ï¸ å®æ–½è·¯çº¿å›¾

### é˜¶æ®µ 1: åŸºç¡€æ¶æ„è¿ç§» (2-3 å‘¨)
- [ ] å®‰è£… LangGraph å’Œç›¸å…³ä¾èµ–
- [ ] é‡æ„ç°æœ‰ä»£ç ä¸º StateGraph å·¥ä½œæµ
- [ ] å®ç°åŸºç¡€çš„å¤š Agent åä½œ
- [ ] æ·»åŠ è¾“å…¥è¾“å‡ºéªŒè¯

### é˜¶æ®µ 2: RAG å’Œè®°å¿†ç³»ç»Ÿ (2-3 å‘¨)  
- [ ] é›†æˆå‘é‡æ•°æ®åº“å’Œ RAG æ£€ç´¢
- [ ] å®ç°åˆ†å±‚è®°å¿†æ¶æ„
- [ ] æ·»åŠ å®æ—¶æ–‡çŒ®æ£€ç´¢åŠŸèƒ½
- [ ] ä¼˜åŒ–çŸ¥è¯†åº“æ›´æ–°æœºåˆ¶

### é˜¶æ®µ 3: å¤šæ¨¡æ€å’Œå‰ç«¯ (2-3 å‘¨)
- [ ] æ·»åŠ å›¾åƒå’Œè¯­éŸ³å¤„ç†èƒ½åŠ›
- [ ] å¼€å‘ Streamlit å‰ç«¯ç•Œé¢
- [ ] å®ç°æ–‡ä»¶ä¸Šä¼ å’Œå¤„ç†
- [ ] ä¼˜åŒ–ç”¨æˆ·ä½“éªŒå’Œäº¤äº’

### é˜¶æ®µ 4: API å’Œéƒ¨ç½² (1-2 å‘¨)
- [ ] å¼€å‘ FastAPI æœåŠ¡æ¥å£
- [ ] æ·»åŠ æ€§èƒ½ç›‘æ§å’Œæ—¥å¿—
- [ ] å®ç°è´Ÿè½½å‡è¡¡å’Œç¼“å­˜
- [ ] éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ

### é˜¶æ®µ 5: è¯„ä¼°å’Œä¼˜åŒ– (æŒç»­)
- [ ] å®ç° ROUGE ç­‰è¯„ä¼°æŒ‡æ ‡
- [ ] æ·»åŠ  A/B æµ‹è¯•æ¡†æ¶
- [ ] æ”¶é›†ç”¨æˆ·åé¦ˆå’Œä¼˜åŒ–
- [ ] æŒç»­æ¨¡å‹å¾®è°ƒå’Œæ”¹è¿›

---

## ğŸ“Š è¯„ä¼°æ–¹æ³•å‡çº§

### å¤šç»´åº¦è¯„ä¼°æ¡†æ¶
```python
from rouge import Rouge
from bert_score import score
import numpy as np

class ComprehensiveEvaluator:
    def __init__(self):
        self.rouge = Rouge()
        
    def evaluate_response(self, 
                         reference: str, 
                         prediction: str,
                         query_type: str) -> Dict[str, float]:
        
        # ROUGE è¯„åˆ†
        rouge_scores = self.rouge.get_scores(prediction, reference)[0]
        
        # BERTScore è¯­ä¹‰ç›¸ä¼¼åº¦
        P, R, F1 = score([prediction], [reference], lang="zh", verbose=False)
        
        # é¢†åŸŸç‰¹å®šè¯„ä¼°
        domain_score = self.evaluate_domain_accuracy(
            prediction, reference, query_type
        )
        
        # å®‰å…¨æ€§è¯„ä¼°
        safety_score = self.evaluate_safety(prediction)
        
        return {
            "rouge_1": rouge_scores['rouge-1']['f'],
            "rouge_2": rouge_scores['rouge-2']['f'], 
            "rouge_l": rouge_scores['rouge-l']['f'],
            "bert_score": F1.mean().item(),
            "domain_accuracy": domain_score,
            "safety_score": safety_score,
            "overall_score": self.calculate_weighted_score({
                "rouge": rouge_scores['rouge-l']['f'],
                "bert": F1.mean().item(),
                "domain": domain_score,
                "safety": safety_score
            })
        }
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### å¹¶è¡Œå¤„ç†ä¼˜åŒ–
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class ParallelProcessor:
    def __init__(self, max_workers: int = 4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    async def parallel_agent_processing(self, 
                                      query: MedicalQuery, 
                                      agents: List[Agent]) -> List[AgentResponse]:
        """å¹¶è¡Œå¤„ç†å¤šä¸ª Agent"""
        tasks = [
            asyncio.create_task(agent.process(query)) 
            for agent in agents
        ]
        
        return await asyncio.gather(*tasks, return_exceptions=True)
    
    async def batch_rag_retrieval(self, 
                                queries: List[str]) -> List[List[Document]]:
        """æ‰¹é‡ RAG æ£€ç´¢"""
        loop = asyncio.get_event_loop()
        
        tasks = [
            loop.run_in_executor(
                self.executor, 
                self.vectorstore.similarity_search, 
                query
            ) 
            for query in queries
        ]
        
        return await asyncio.gather(*tasks)
```

### æ™ºèƒ½ç¼“å­˜ç­–ç•¥
```python
from functools import lru_cache
import hashlib

class SmartCache:
    def __init__(self):
        self.redis_client = redis.Redis()
    
    def cache_key(self, query: str, context: dict) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{query}:{str(context)}"
        return hashlib.md5(content.encode()).hexdigest()
    
    @lru_cache(maxsize=1000)
    def get_cached_response(self, cache_key: str) -> Optional[MedicalResponse]:
        """è·å–ç¼“å­˜å“åº”"""
        cached = self.redis_client.get(f"response:{cache_key}")
        if cached:
            return MedicalResponse.parse_raw(cached)
        return None
    
    def cache_response(self, 
                      cache_key: str, 
                      response: MedicalResponse,
                      ttl: int = 3600):
        """ç¼“å­˜å“åº”"""
        self.redis_client.setex(
            f"response:{cache_key}", 
            ttl, 
            response.json()
        )
```

---

è¿™ä¸ªä¼˜åŒ–æŒ‡å—åŸºäºä¹å¤§æ­¥éª¤æ¡†æ¶ï¼Œæä¾›äº†ä»æ¶æ„é‡è®¾è®¡åˆ°éƒ¨ç½²ä¸Šçº¿çš„å®Œæ•´æ–¹æ¡ˆã€‚é‡ç‚¹å…³æ³¨ LangGraph å·¥ä½œæµã€å¤šæ¨¡æ€èƒ½åŠ›ã€RAG æŠ€æœ¯é›†æˆå’Œç”Ÿäº§å°±ç»ªçš„éƒ¨ç½²æ–¹æ¡ˆã€‚

å»ºè®®æŒ‰ç…§é˜¶æ®µæ€§è·¯çº¿å›¾é€æ­¥å®æ–½ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰æ˜ç¡®çš„äº¤ä»˜æˆæœå’Œæµ‹è¯•æ ‡å‡†ã€‚